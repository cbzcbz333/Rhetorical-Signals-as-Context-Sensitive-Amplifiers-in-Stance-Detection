# features/rhetoric.py
import re
from typing import List, Dict, Any
from feature_extractor import OptimizedFeatureExtractor
import requests
import json
import re
import time
from typing import Dict, List, Any, Optional, Tuple
import logging
from dataclasses import dataclass
from datetime import datetime
import hashlib
from contrast_extractor import extract_contrast_features

def simple_length_norm(text: str) -> float:
    # ä½ å¯ä»¥æ¢æˆ tokenizer é•¿åº¦ï¼Œè¿™é‡Œç”¨å­—ç¬¦é•¿åº¦åšæœ€ç®€å•å½’ä¸€åŒ–
    return max(1.0, len(text))

class RhetoricFeatureEncoder:
    """
    æŠŠä½ ç°æœ‰ LLM æŠ½å–ç»“æœ -> å®šé•¿ rf å‘é‡
    """
    def __init__(self, api_key: str = None, cache_enabled: bool = True):
        self.ext = OptimizedFeatureExtractor(api_key=api_key, cache_enabled=cache_enabled)

    def extract_vector(self, text: str) -> List[float]:
        L = simple_length_norm(text)

        # 1) RQ
        rq = self.ext.extract_features(text, feature_type="rhetorical", use_cache=True)
        rq_q = rq.question_count
        rq_r = rq.rhetorical_count
        rq_ratio = rq.rhetorical_ratio

        # 2) MOD
        mod = self.ext.extract_features(text, feature_type="modality", use_cache=True)
        modal = float(mod.get("modal_verb_count", 0))
        hedge = float(mod.get("hedge_marker_count", 0))
        strong = float(mod.get("strong_assertion_count", 0))
        epi = float(mod.get("epistemic_strength_score", 0.0))

        # 3) CON (å¦‚æœ contrast_extractor æœ‰)
        # è¿™é‡Œç»™ä¸€ä¸ªå¾ˆä¿å®ˆçš„å†™æ³•ï¼šæ‹¿ä¸åˆ°å°±ç½®0
        try:
            from contrast_extractor import extract_contrast_features
            con = extract_contrast_features(text)
            # ä½ éœ€è¦æŠŠå®ƒæ˜ å°„æˆä¸€ä¸ªæ•°ï¼šæ¯”å¦‚ con["contrast_count"]/L æˆ– con["has_contrast"]
            con_score = float(con.get("contrast_score", con.get("has_contrast", 0.0)))
        except Exception:
            con_score = 0.0

        # å½’ä¸€åŒ–ï¼ˆè®¡æ•°/Lï¼‰
        rf = [
            rq_q / L,
            rq_r / L,
            rq_ratio,
            modal / L,
            hedge / L,
            strong / L,
            epi,
            con_score
        ]
        return rf

    @property
    def rf_dim(self) -> int:
        return 8

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class ExtractionResult:
    """ç‰¹å¾æå–ç»“æœ"""
    text: str
    question_count: int
    rhetorical_count: int
    rhetorical_ratio: float
    questions: List[Dict]
    metadata: Dict
    raw_response: Optional[str] = None

    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "text": self.text,
            "features": {
                "question_count": self.question_count,
                "rhetorical_question_count": self.rhetorical_count,
                "rhetorical_question_ratio": self.rhetorical_ratio
            },
            "analysis": {
                "questions": self.questions,
                "raw_response": self.raw_response
            },
            "metadata": self.metadata
        }

    def summary(self) -> str:
        """ç»“æœæ‘˜è¦"""
        return (f"æ–‡æœ¬: {self.text[:50]}...\n"
                f"ç–‘é—®å¥: {self.question_count} | "
                f"åé—®å¥: {self.rhetorical_count} | "
                f"æ¯”ä¾‹: {self.rhetorical_ratio:.2f} | "
                f"æ¨¡å¼: {self.metadata.get('method', 'unknown')}")


class OptimizedFeatureExtractor:
    """ä¼˜åŒ–ç‰ˆç‰¹å¾æå–å™¨ - ä¸“é—¨ç”¨äºç«‹åœºæ£€æµ‹"""

    def __init__(self, api_key: str = None, cache_enabled: bool = True):
        self.api_key = api_key or "sk-JhzsitLNi4ztobLxgmbdIBCPXtUPTFwFmkYdAsOILqW1xDEy"
        self.endpoint = "https://api.shubiaobiao.com/v1/chat/completions"

        # å·²ç¡®è®¤å¯ç”¨çš„æ¨¡å‹ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        self.models = [
            "gpt-4o-mini",  # æˆæœ¬ä½ï¼Œé€Ÿåº¦å¿«
            "gpt-3.5-turbo",  # æœ€ç¨³å®š
            "gpt-4o",  # èƒ½åŠ›å¼º
            "gpt-4.1-mini",  # è¾ƒæ–°ç‰ˆæœ¬
            "gpt-3.5-turbo-0125",  # ç‰¹å®šç‰ˆæœ¬
        ]

        # ç¼“å­˜ç³»ç»Ÿ
        self.cache_enabled = cache_enabled
        self.response_cache: Dict[str, Dict] = {}
        self.cache_ttl = 3600  # 1å°æ—¶

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "cache_hits": 0,
            "total_tokens": 0,
            "models_used": {}
        }

        logger.info(f"âœ… ç‰¹å¾æå–å™¨åˆå§‹åŒ–å®Œæˆï¼Œå¯ç”¨æ¨¡å‹: {self.models}")

    def _get_cache_key(self, text: str, feature_type: str = "rhetorical") -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = f"{feature_type}:{text}"
        return hashlib.md5(content.encode()).hexdigest()

    def _save_to_cache(self, key: str, data: Dict):
        """ä¿å­˜åˆ°ç¼“å­˜"""
        if self.cache_enabled:
            self.response_cache[key] = {
                "data": data,
                "timestamp": time.time()
            }

    def _get_from_cache(self, key: str) -> Optional[Dict]:
        """ä»ç¼“å­˜è·å–"""
        if not self.cache_enabled:
            return None

        if key in self.response_cache:
            entry = self.response_cache[key]
            if time.time() - entry["timestamp"] < self.cache_ttl:
                self.stats["cache_hits"] += 1
                return entry["data"]

        return None

    def _build_system_prompt(self, feature_type: str = "rhetorical") -> str:
        """æ„å»ºç³»ç»Ÿæç¤ºè¯ï¼ˆæŒ‰ä¿®è¾ç‰¹å¾ç±»å‹ï¼‰"""

        prompts = {

            # =========================
            # ç¬¬ 1 ç±»ï¼šåé—® / è®¾é—®
            # =========================
            "rhetorical": """You are a linguistic analysis assistant.
    Your task is to identify rhetorical questions in short social media text.

    Definition:
    A rhetorical question is an interrogative form that does NOT seek information,
    but is used to implicitly express emphasis, criticism, or evaluation.

    Instructions:
    1. Identify all complete question sentences in the text.
    2. For each question, determine whether it is rhetorical.
    3. Do NOT infer stance polarity or sentiment.
    4. Focus only on the rhetorical function of the question.

    Criteria for rhetorical questions (any one is sufficient):
    - The question implies an obvious or assumed answer.
    - The question is used for emphasis rather than information seeking.
    - Typical rhetorical markers are present.

    Examples of rhetorical markers:
    - Chinese: éš¾é“, å²‚, ä½•å¿…, ä¸æ˜¯â€¦å—, æ€èƒ½, æ€å¯
    - English: isn't it, don't you, how can, why would

    Output:
    Return ONLY a valid JSON object in the following format:

    {
      "question_count": <int>,
      "rhetorical_question_count": <int>,
      "questions": [
        {
          "text": "<question sentence>",
          "is_rhetorical": true/false
        }
      ]
    }
    """,

            # =========================
            # ç¬¬ 2 ç±»ï¼šæ¨¡æ€ / æ¨¡ç³Šè¡¨è¾¾
            # =========================
            "modality": """You are a linguistic analysis assistant.
    Your task is to identify modality and hedging in short social media text.

    Definition:
    Modality and hedging reflect the degree of certainty, obligation, or commitment
    expressed by the speaker, rather than sentiment or stance direction.

    Instructions:
    1. Identify modal verbs indicating obligation or possibility.
    2. Identify hedging expressions that soften or qualify claims.
    3. Identify strong assertion markers indicating high certainty.
    4. Do NOT infer stance polarity or sentiment.

    Examples:
    - Modal verbs: should, could, might, may, must
    - Hedging expressions: maybe, perhaps, it seems, I think, likely
    - Strong assertions: must, definitely, obviously, no doubt

    Output:
    Return ONLY a valid JSON object in the following format:

    {
      "modal_verb_count": <int>,
      "hedge_marker_count": <int>,
      "strong_assertion_count": <int>
    }
    """
        }

        return prompts.get(feature_type, prompts["rhetorical"])

    def _call_api_with_retry(self, messages: List[Dict], max_retries: int = 3) -> Tuple[bool, Any]:
        """å¸¦é‡è¯•çš„APIè°ƒç”¨"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        for model in self.models:
            for attempt in range(max_retries):
                try:
                    self.stats["total_requests"] += 1

                    payload = {
                        "model": model,
                        "messages": messages,
                        "temperature": 0,
                        "max_tokens": 1000,
                        "response_format": {"type": "json_object"}
                    }

                    logger.debug(f"å°è¯•æ¨¡å‹: {model} (å°è¯• {attempt + 1}/{max_retries})")

                    start_time = time.time()
                    response = requests.post(
                        self.endpoint,
                        headers=headers,
                        json=payload,
                        timeout=15
                    )
                    response_time = time.time() - start_time

                    if response.status_code == 200:
                        data = response.json()

                        # è®°å½•ä½¿ç”¨ç»Ÿè®¡
                        if model not in self.stats["models_used"]:
                            self.stats["models_used"][model] = 0
                        self.stats["models_used"][model] += 1

                        # è®°å½•tokenä½¿ç”¨ï¼ˆå¦‚æœæœ‰ï¼‰
                        if "usage" in data:
                            self.stats["total_tokens"] += data["usage"].get("total_tokens", 0)

                        self.stats["successful_requests"] += 1
                        logger.debug(f"âœ… {model} æˆåŠŸ ({response_time:.2f}s)")

                        return True, {
                            "data": data,
                            "model": model,
                            "response_time": response_time
                        }

                    else:
                        error_msg = response.text[:100] if response.text else ""
                        logger.warning(f"âŒ {model} å¤±è´¥: {response.status_code} - {error_msg}")

                        # å¦‚æœæ˜¯ä¸´æ—¶é”™è¯¯ï¼Œé‡è¯•
                        if response.status_code in [429, 500, 502, 503, 504]:
                            wait_time = (attempt + 1) * 2  # æŒ‡æ•°é€€é¿
                            logger.info(f"ç­‰å¾… {wait_time}s åé‡è¯•...")
                            time.sleep(wait_time)
                            continue
                        else:
                            # æ°¸ä¹…é”™è¯¯ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹
                            break

                except requests.exceptions.Timeout:
                    logger.warning(f"â±ï¸  {model} è¶…æ—¶")
                    if attempt < max_retries - 1:
                        time.sleep(2)
                        continue
                except Exception as e:
                    logger.error(f"ğŸ’¥ {model} å¼‚å¸¸: {e}")
                    break

        self.stats["failed_requests"] += 1
        return False, {"error": "æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥"}

    def extract_features(
            self,
            text: str,
            feature_type: str = "rhetorical",
            use_cache: bool = True
    ):
        """
        ä¿®è¾ç‰¹å¾æå–ä¸»å…¥å£ï¼ˆæ”¯æŒå¤šç±»ä¿®è¾ï¼‰

        Args:
            text: è¾“å…¥æ–‡æœ¬
            feature_type: ä¿®è¾ç±»å‹ï¼ˆrhetorical / modality / ...ï¼‰
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜

        Returns:
            - rhetorical: ExtractionResult
            - modality: dict
        """

        # =========================
        # ç¬¬ 1 ç±»ï¼šåé—® / è®¾é—®
        # =========================
        if feature_type == "rhetorical":

            # ---- ç¼“å­˜ ----
            if use_cache and self.cache_enabled:
                cache_key = self._get_cache_key(text, "rhetorical")
                cached = self._get_from_cache(cache_key)
                if cached:
                    return ExtractionResult(
                        text=cached["text"],
                        question_count=cached["question_count"],
                        rhetorical_count=cached["rhetorical_question_count"],
                        rhetorical_ratio=cached["rhetorical_question_ratio"],
                        questions=cached.get("questions", []),
                        metadata=cached.get("metadata", {}),
                        raw_response=cached.get("raw_response")
                    )

            # ---- æ„å»ºæ¶ˆæ¯ ----
            messages = [
                {"role": "system", "content": self._build_system_prompt("rhetorical")},
                {"role": "user", "content": f'Text:\n"{text}"'}
            ]

            # ---- è°ƒç”¨ API ----
            success, result = self._call_api_with_retry(messages)

            if success:
                try:
                    data = result["data"]
                    content = data["choices"][0]["message"]["content"]

                    features = json.loads(content)
                    features = self._validate_features(features, text)

                    extraction_result = ExtractionResult(
                        text=text,
                        question_count=features.get("question_count", 0),
                        rhetorical_count=features.get("rhetorical_question_count", 0),
                        rhetorical_ratio=features.get("rhetorical_question_ratio", 0.0),
                        questions=features.get("questions", []),
                        metadata={
                            "model": result["model"],
                            "method": "api",
                            "response_time": result["response_time"],
                            "success": True,
                            "timestamp": datetime.now().isoformat(),
                            "feature_type": "rhetorical"
                        },
                        raw_response=content[:500] + "..." if len(content) > 500 else content
                    )

                    if use_cache and self.cache_enabled:
                        cache_key = self._get_cache_key(text, "rhetorical")
                        self._save_to_cache(cache_key, extraction_result.to_dict())

                    return extraction_result

                except Exception as e:
                    logger.error(f"åé—®ç‰¹å¾å¤„ç†å¤±è´¥: {e}")
                    return self._create_fallback_result(text, str(e))

            else:
                logger.warning("åé—® API å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™åŒ¹é…")
                return self._rule_based_extraction(text)

        # =========================
        # ç¬¬ 2 ç±»ï¼šæ¨¡æ€ / æ¨¡ç³Šè¡¨è¾¾
        # =========================
        elif feature_type == "modality":

            # ---- ç¼“å­˜ï¼ˆç‹¬ç«‹ keyï¼Œä¸æ±¡æŸ“ç¬¬ 1 ç±»ï¼‰----
            if use_cache and self.cache_enabled:
                cache_key = self._get_cache_key(text, "modality")
                cached = self._get_from_cache(cache_key)
                if cached:
                    return cached

            messages = [
                {"role": "system", "content": self._build_system_prompt("modality")},
                {"role": "user", "content": f'Text:\n"{text}"'}
            ]

            success, result = self._call_api_with_retry(messages)

            if success:
                try:
                    content = result["data"]["choices"][0]["message"]["content"]
                    data = json.loads(content)

                    modal = int(data.get("modal_verb_count", 0))
                    hedge = int(data.get("hedge_marker_count", 0))
                    strong = int(data.get("strong_assertion_count", 0))

                    denom = modal + hedge + strong
                    strength = (strong - hedge) / denom if denom > 0 else 0.0

                    features = {
                        "modal_verb_count": modal,
                        "hedge_marker_count": hedge,
                        "strong_assertion_count": strong,
                        "epistemic_strength_score": round(strength, 4)
                    }

                    if use_cache and self.cache_enabled:
                        cache_key = self._get_cache_key(text, "modality")
                        self._save_to_cache(cache_key, features)

                    return features

                except Exception as e:
                    logger.error(f"æ¨¡æ€ç‰¹å¾å¤„ç†å¤±è´¥: {e}")
                    return {
                        "modal_verb_count": 0,
                        "hedge_marker_count": 0,
                        "strong_assertion_count": 0,
                        "epistemic_strength_score": 0.0
                    }

            else:
                logger.warning("æ¨¡æ€ API å¤±è´¥ï¼Œä½¿ç”¨ç©ºç‰¹å¾")
                return {
                    "modal_verb_count": 0,
                    "hedge_marker_count": 0,
                    "strong_assertion_count": 0,
                    "epistemic_strength_score": 0.0
                }

        # =========================
        # æœªçŸ¥ç±»å‹ï¼ˆé˜²å¾¡ï¼‰
        # =========================
        else:
            raise ValueError(f"Unknown feature_type: {feature_type}")

    def _validate_features(self, features: Dict, original_text: str) -> Dict:
        """éªŒè¯å’Œè¡¥å…¨ç‰¹å¾"""
        # åŸºæœ¬éªŒè¯
        if not isinstance(features, dict):
            features = {}

        # ç¡®ä¿å¿…éœ€å­—æ®µ
        features.setdefault("question_count", 0)
        features.setdefault("rhetorical_question_count", 0)
        features.setdefault("questions", [])

        # è®¡ç®—æ¯”ä¾‹
        qc = features["question_count"]
        rc = features["rhetorical_question_count"]
        features["rhetorical_question_ratio"] = rc / qc if qc > 0 else 0.0

        # éªŒè¯questionsæ•°ç»„
        if not isinstance(features["questions"], list):
            features["questions"] = []

        # æ¸…ç†æ¯ä¸ªé—®é¢˜æ¡ç›®
        valid_questions = []
        for q in features["questions"]:
            if isinstance(q, dict):
                # ç¡®ä¿å¿…éœ€å­—æ®µ
                q.setdefault("text", "")
                q.setdefault("is_rhetorical", False)


                # ç¡®ä¿ç½®ä¿¡åº¦åœ¨0-1ä¹‹é—´
                '''
                                if not isinstance(q["confidence"], (int, float)):
                    q["confidence"] = 0.5
                q["confidence"] = max(0.0, min(1.0, float(q["confidence"])))
                '''


                valid_questions.append(q)

        features["questions"] = valid_questions

        # å¦‚æœquestionsæ•°é‡ä¸ç»Ÿè®¡ä¸ä¸€è‡´ï¼Œä¿®æ­£ç»Ÿè®¡
        actual_qc = len(features["questions"])
        if actual_qc != features["question_count"]:
            logger.debug(f"ä¿®æ­£question_count: {features['question_count']} -> {actual_qc}")
            features["question_count"] = actual_qc

        actual_rc = sum(1 for q in features["questions"] if q["is_rhetorical"] is True)

        if actual_rc != features["rhetorical_question_count"]:
            logger.debug(f"ä¿®æ­£rhetorical_count: {features['rhetorical_question_count']} -> {actual_rc}")
            features["rhetorical_question_count"] = actual_rc

        # é‡æ–°è®¡ç®—æ¯”ä¾‹
        qc = features["question_count"]
        rc = features["rhetorical_question_count"]
        features["rhetorical_question_ratio"] = rc / qc if qc > 0 else 0.0

        return features

    def _rule_based_extraction(self, text: str) -> ExtractionResult:
        """åŸºäºè§„åˆ™çš„é™çº§ç‰¹å¾æå–"""
        logger.info(f"ä½¿ç”¨è§„åˆ™åŒ¹é…: {text[:50]}...")

        # ä¼˜åŒ–çš„åé—®å¥æ¨¡å¼
        rhetorical_patterns = [
            # ä¸­æ–‡æ¨¡å¼
            (r'éš¾é“[^ï¼Ÿ?]*[ï¼Ÿ?]', "å«æœ‰'éš¾é“'"),
            (r'å²‚[^ï¼Ÿ?]*[ï¼Ÿ?]', "å«æœ‰'å²‚'"),
            (r'ä½•å°[^ï¼Ÿ?]*[ï¼Ÿ?]', "å«æœ‰'ä½•å°'"),
            (r'å²‚ä¸æ˜¯[^ï¼Ÿ?]*[ï¼Ÿ?]', "å«æœ‰'å²‚ä¸æ˜¯'"),
            (r'æ€ä¹ˆ(?:èƒ½|å¯ä»¥|å¯èƒ½|æ•¢|ä¼š)[^ï¼Ÿ?]*[ï¼Ÿ?]', "å«æœ‰'æ€ä¹ˆ...'ç»“æ„"),
            (r'æ€èƒ½[^ï¼Ÿ?]*[ï¼Ÿ?]', "å«æœ‰'æ€èƒ½'"),
            (r'æ€å¯[^ï¼Ÿ?]*[ï¼Ÿ?]', "å«æœ‰'æ€å¯'"),
            (r'ä½•å¿…[^ï¼Ÿ?]*[ï¼Ÿ?]', "å«æœ‰'ä½•å¿…'"),
            (r'ä¸æ˜¯[^ï¼Ÿ?]*å—[ï¼Ÿ?]', "'ä¸æ˜¯...å—'ç»“æ„"),
            (r'è¿˜ä¸[^ï¼Ÿ?]*å—[ï¼Ÿ?]', "'è¿˜ä¸...å—'ç»“æ„"),
            (r'æ²¡æœ‰[^ï¼Ÿ?]*å—[ï¼Ÿ?]', "'æ²¡æœ‰...å—'ç»“æ„"),

            # è‹±æ–‡æ¨¡å¼ï¼ˆä¿®å¤é—®é¢˜ï¼‰
            (r'(?:isn\'t|aren\'t|don\'t|doesn\'t|won\'t|can\'t)\s+it\b.*[ï¼Ÿ?]', "è‹±æ–‡åé—®: isn't itç»“æ„"),
            (r'(?:isn\'t|aren\'t|don\'t|doesn\'t|won\'t|can\'t)\s+.*\b\?', "è‹±æ–‡åé—®: å¦å®šç–‘é—®"),
            (r'how\s+(?:can|could|dare|would)\s+.*[ï¼Ÿ?]', "è‹±æ–‡åé—®: how canç»“æ„"),
            (r'why\s+(?:would|should)\s+.*[ï¼Ÿ?]', "è‹±æ–‡åé—®: why wouldç»“æ„"),
            (r'what\s+(?:is|are)\s+the\s+point.*[ï¼Ÿ?]', "è‹±æ–‡åé—®: what's the point"),
            (r'who\s+(?:would|could)\s+.*[ï¼Ÿ?]', "è‹±æ–‡åé—®: who would"),
        ]

        # åˆ†å‰²å¥å­
        sentences = re.split(r'[ã€‚ï¼!ï¼›;\n]', text)

        questions = []
        question_count = 0
        rhetorical_count = 0

        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue

            # æ£€æŸ¥æ˜¯å¦æ˜¯ç–‘é—®å¥
            if sentence.endswith('ï¼Ÿ') or sentence.endswith('?'):
                question_count += 1

                is_rhetorical = False
                reason = "æ™®é€šç–‘é—®å¥"
                #confidence = 0.3

                # æ£€æŸ¥åé—®å¥æ¨¡å¼
                for pattern, pattern_reason in rhetorical_patterns:
                    if re.search(pattern, sentence, re.IGNORECASE):
                        is_rhetorical = True
                        reason = f"åŒ¹é…åé—®æ¨¡å¼: {pattern_reason}"
                        #confidence = 0.85
                        rhetorical_count += 1
                        break

                # ç¡®å®šä½ç½®
                if question_count == 1:
                    position = "å¼€å¤´"
                elif question_count == len([s for s in sentences if s.endswith('ï¼Ÿ') or s.endswith('?')]):
                    position = "ç»“å°¾"
                else:
                    position = "ä¸­é—´"

                questions.append({
                    "text": sentence,
                    "is_rhetorical": is_rhetorical
                })

        # åˆ›å»ºç»“æœ
        rhetorical_ratio = rhetorical_count / question_count if question_count > 0 else 0.0

        return ExtractionResult(
            text=text,
            question_count=question_count,
            rhetorical_count=rhetorical_count,
            rhetorical_ratio=rhetorical_ratio,
            questions=questions,
            metadata={
                "model": "rule_based",
                "method": "rule_based",
                "success": True,
                "timestamp": datetime.now().isoformat(),
                "feature_type": "rhetorical",
                "note": "åŸºäºè§„åˆ™çš„é™çº§åˆ†æ"
            }
        )

    def _create_fallback_result(self, text: str, error: str) -> ExtractionResult:
        """åˆ›å»ºé™çº§ç»“æœ"""
        return ExtractionResult(
            text=text,
            question_count=0,
            rhetorical_count=0,
            rhetorical_ratio=0.0,
            questions=[],
            metadata={
                "model": "error",
                "method": "error",
                "success": False,
                "timestamp": datetime.now().isoformat(),
                "error": error,
                "feature_type": "rhetorical"
            }
        )

    def extract_multiple_features(self, text: str, feature_types: List[str] = None) -> Dict[str, Any]:
        """æå–å¤šç§ç‰¹å¾"""
        if feature_types is None:
            feature_types = ["rhetorical"]

        results = {}
        for feature_type in feature_types:
            if feature_type == "rhetorical":
                results[feature_type] = self.extract_features(text).to_dict()
            # å¯ä»¥æ·»åŠ å…¶ä»–ç‰¹å¾ç±»å‹

        return results

    def batch_extract(self, texts: List[str], batch_size: int = 10) -> List[ExtractionResult]:
        """æ‰¹é‡æå–ç‰¹å¾"""
        results = []

        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            logger.info(f"å¤„ç†æ‰¹æ¬¡ {i // batch_size + 1}: {len(batch)} ä¸ªæ–‡æœ¬")

            for text in batch:
                result = self.extract_features(text)
                results.append(result)

            # æ‰¹æ¬¡é—´å»¶è¿Ÿ
            if i + batch_size < len(texts):
                time.sleep(1)

        return results

    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        success_rate = (self.stats["successful_requests"] / self.stats["total_requests"]
                        if self.stats["total_requests"] > 0 else 0)

        return {
            **self.stats,
            "success_rate": success_rate,
            "cache_enabled": self.cache_enabled,
            "cache_size": len(self.response_cache),
            "current_time": datetime.now().isoformat()
        }

def _extract_modality_features(self, text: str) -> dict:
    messages = [
        {
            "role": "system",
            "content": self._build_system_prompt("modality")
        },
        {
            "role": "user",
            "content": f'Text:\n"{text}"'
        }
    ]

    success, result = self._call_api_with_retry(messages)

    if not success:
        return {
            "modal_verb_count": 0,
            "hedge_marker_count": 0,
            "strong_assertion_count": 0,
            "epistemic_strength_score": 0.0
        }

    content = result["data"]["choices"][0]["message"]["content"]
    data = json.loads(content)

    modal = int(data.get("modal_verb_count", 0))
    hedge = int(data.get("hedge_marker_count", 0))
    strong = int(data.get("strong_assertion_count", 0))

    # æ´¾ç”Ÿå¼ºåº¦æŒ‡æ ‡ï¼ˆç®€å•ã€å¯è§£é‡Šï¼‰
    denom = modal + hedge + strong
    strength = (strong - hedge) / denom if denom > 0 else 0.0

    return {
        "modal_verb_count": modal,
        "hedge_marker_count": hedge,
        "strong_assertion_count": strong,
        "epistemic_strength_score": round(strength, 4)
    }

# æµ‹è¯•å’Œä½¿ç”¨ç¤ºä¾‹
def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¯åŠ¨ä¼˜åŒ–ç‰ˆç‰¹å¾æå–å™¨...")
    print("=" * 70)

    # åˆ›å»ºæå–å™¨
    extractor = OptimizedFeatureExtractor(cache_enabled=True)

    # æµ‹è¯•æ–‡æœ¬ï¼ˆåŒ…å«ä¹‹å‰è¯†åˆ«é”™è¯¯çš„è‹±æ–‡åé—®å¥ï¼‰
    test_texts = [
        "éš¾é“ä½ ä¸çŸ¥é“è¿™ä¸ªè§„å®šå—ï¼Ÿä¸ºä»€ä¹ˆè¿˜è¦è¿™æ ·åšï¼Ÿ",
        "Isn't it beautiful? What do you think?",
        "è¿™éš¾é“ä¸æ˜¯ä½ çš„è´£ä»»å—ï¼Ÿä½ æ€ä¹ˆèƒ½æ¨å¸ï¼Ÿ",
        "How can you say that? Doesn't it make sense?",
        "ä»Šå¤©å¤©æ°”çœŸå¥½ï¼Œæˆ‘ä»¬å»å…¬å›­å§ï¼Ÿ",
        "ä½ è¿˜æ²¡æœ‰å®Œæˆä½œä¸šå—ï¼Ÿè¿™æ€ä¹ˆè¡Œï¼Ÿ",
    ]

    print(f"æµ‹è¯• {len(test_texts)} ä¸ªæ–‡æœ¬...")
    print("-" * 70)

    # æ‰¹é‡æå–
    results = extractor.batch_extract(test_texts, batch_size=3)

    # æ˜¾ç¤ºç»“æœ
    for i, result in enumerate(results, 1):
        print(f"\n{i}. {result.summary()}")

        if result.questions:
            print("  è¯¦ç»†åˆ†æ:")
            for j, q in enumerate(result.questions, 1):
                marker = "ğŸ”¸" if q["is_rhetorical"] else "â—¦"
                print(f"    {marker} é—®é¢˜{j}: {q['text']}")
                """
                                if q["is_rhetorical"]:
                    print(f"       ç†ç”±: {q['reason']} (ç½®ä¿¡åº¦: {q['confidence']:.2f})")
                """


        print(f"  æ¨¡å‹: {result.metadata.get('model')} | "
              f"ç”¨æ—¶: {result.metadata.get('response_time', 0):.2f}s")

    # æ˜¾ç¤ºç»Ÿè®¡
    print("\n" + "=" * 70)
    print("ä½¿ç”¨ç»Ÿè®¡:")
    stats = extractor.get_stats()
    print(f"æ€»è¯·æ±‚: {stats['total_requests']}")
    print(f"æˆåŠŸ: {stats['successful_requests']}")
    print(f"å¤±è´¥: {stats['failed_requests']}")
    print(f"æˆåŠŸç‡: {stats['success_rate']:.2%}")
    print(f"ç¼“å­˜å‘½ä¸­: {stats['cache_hits']}")
    print(f"æ€»token: {stats['total_tokens']}")

    if stats["models_used"]:
        print("æ¨¡å‹ä½¿ç”¨æƒ…å†µ:")
        for model, count in stats["models_used"].items():
            print(f"  {model}: {count}æ¬¡")


if __name__ == "__main__":
    main()